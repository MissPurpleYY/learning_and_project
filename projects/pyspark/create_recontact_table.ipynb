{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext , Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import date\n",
    "\n",
    "from pyspark.sql.functions import col, expr, udf, coalesce, lit\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = '*****@***.com'\n",
    "APP_NAME = 'customer_service_recontact_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spark_context():\n",
    "    return (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(APP_NAME)\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\")\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "        .config(\"hive.exec.max.dynamic.partitions\", \"100000\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_logger(spark):\n",
    "    log4j = spark.sparkContext._jvm.org.apache.log4j\n",
    "    logger = log4j.LogManager.getLogger(spark.sparkContext.appName)\n",
    "    logger.setLevel(log4j.Level.INFO)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def get_data(spark):\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT action_id,\n",
    "               ticket_id,\n",
    "               a.reservation_id,\n",
    "               a.created_date AS contact_date,\n",
    "               channel,\n",
    "               checkin,\n",
    "               topic,\n",
    "               subtopic,\n",
    "               booking_window,\n",
    "               DATEDIFF(checkin, a.created_date) AS contact_days_before_checkin\n",
    "        FROM table_contact a\n",
    "        JOIN table_reservation r\n",
    "            ON a.reservation_id = r.reservation_id\n",
    "        WHERE has_staff_interaction == true\n",
    "        AND direction =='inbound'\n",
    "        \"\"\"\n",
    "    df = spark.sql(query).cache()\n",
    "    df = df.withColumn(\"date\",expr(\"to_date(contact_date, 'yyyy-mm-dd')\"))\n",
    "    df = df.withColumn('timestamp',df.date.astype('Timestamp').cast(\"long\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "def window_groupby(df):\n",
    "    days = lambda i: i * 96400\n",
    "    \n",
    "    recontact_same_day = Window.partitionBy('ticket_id').orderBy(sf.asc('date')).rangeBetween(0,0)\n",
    "    recontact_within_3days = Window.partitionBy('ticket_id').orderBy('timestamp').rangeBetween(-days(3),0)\n",
    "    recontact_within_7days = Window.partitionBy('ticket_id').orderBy('timestamp').rangeBetween(-days(7),0)\n",
    "    \n",
    "    df_recontact = (\n",
    "    df\n",
    "    .withColumn('7day_recontact',sf.count('action_id').over(recontact_within_7days))\n",
    "    .withColumn('3day_recontact',sf.count('action_id').over(recontact_within_3days))\n",
    "    .withColumn('sameday_recontact', sf.count('action_id').over(recontact_same_day))\n",
    "    )\n",
    "    \n",
    "    df_recontact = df_recontact.drop('action_id','channel').dropDuplicates()\n",
    "    \n",
    "    return df_recontact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_hadoop(spark_df, tableName, writeMode):\n",
    "    \n",
    "    spark_df.write.saveAsTable(tableName, format='orc', mode = writeMode)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = create_spark_context()\n",
    "    logger = get_logger(spark)\n",
    "\n",
    "    logger.info('Starting')\n",
    "    logger.info('Getting source data on contact level')\n",
    "    df = get_data(spark)\n",
    "    \n",
    "    logger.info('Applying window function to group by recontacts')\n",
    "    df_recontact = window_groupby(df)\n",
    "    logger.info('Get same day, 3 days, 7 days recontacts')\n",
    "    \n",
    "    logger.info('Create binary indicator')\n",
    "    df_recontact = df_recontact.withColumn('duration_3day',\n",
    "                                       sf.when(sf.col('sameday_recontact') < sf.col('3day_recontact'),\n",
    "                                               1).otherwise(0))\n",
    "    df_recontact = df_recontact.withColumn('duration_7day',\n",
    "                                       sf.when(sf.col('3day_recontact') < sf.col('7day_recontact'),\n",
    "                                               1).otherwise(0))\n",
    "    \n",
    "    logger.info('write spark dataframe to Hadoop')\n",
    "    print('Writing data to Hive')\n",
    "    write_to_hadoop(df_recontact, 'table_recontact', 'overwrite')\n",
    "    \n",
    "    print('DONE !')\n",
    "    logger.info('Finished')\n",
    "\n",
    "    \n",
    "    return df_recontact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
